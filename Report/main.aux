\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {0}Abstract}{1}{section.0}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0}Acknowledgments}{2}{section.0}\protected@file@percent }
\citation{Efficient Content-Based Retrieval of Motion Capture Data}
\citation{Exploiting temporal information for 3D pose estimation}
\citation{3D Human Pose Estimation from Deep Multi-View 2D Pose}
\citation{3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The increased demand of mocap clips}{3}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Thesis aim}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Thesis Structure}{4}{subsection.1.3}\protected@file@percent }
\citation{Review on Motion Capture Technology}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Mocap clips}{6}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \href  {https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRNmJKWXW9WvSq-LKojP9PJvF245HXHpa7DUA&usqp=CAU} {MoCap systems Hierarchy}\relax }}{6}{figure.caption.1}\protected@file@percent }
\citation{Optical Motion Capture: Theory and Implementation}
\citation{MOTION CAPTURE TO BUILD A FOUNDATION FOR A COMPUTER-CONTROLLED INSTRUMENT BY STUDY OF CLASSICAL GUITAR PERFORMANCE}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Optical Motion Capture}{7}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \href  {https://www.researchgate.net/profile/Jacek-Hordyj/publication/283152771/figure/fig1/AS:669997391159296@1536751234023/Actor-wearing-suit-adjusted-for-optical-motion-capture-on-the-left-Virtual-model.png} {Optical Motion Capture suit}\relax }}{7}{figure.caption.2}\protected@file@percent }
\citation{Kalman Filtering for Sensor Fusionin a Human Tracking System}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Non-Optical Motion Capture}{8}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \href  {https://www.researchgate.net/profile/Matthew-Field-6/publication/257308000/figure/fig1/AS:613448983531582@1523269043700/a-The-inertial-sensor-MTx-left-14-and-positioning-of-the-sensors-and-wireless.png} {Inertial sensor suit}\relax }}{8}{figure.caption.3}\protected@file@percent }
\citation{MOTION CAPTURE TO BUILD A FOUNDATION FOR A COMPUTER-CONTROLLED INSTRUMENT BY STUDY OF CLASSICAL GUITAR PERFORMANCE}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \href  {https://metamotion.com/images/gypsy4_standing.jpg} {Mechanical mocap suit}\relax }}{9}{figure.caption.4}\protected@file@percent }
\citation{MOTION CAPTURE TO BUILD A FOUNDATION FOR A COMPUTER-CONTROLLED INSTRUMENT BY STUDY OF CLASSICAL GUITAR PERFORMANCE}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \href  {https://www.researchgate.net/profile/Jessica-Hodgins-2/publication/2359279/figure/fig4/AS:669524957331457@1536638597171/A-performer-wearing-a-motion-capture-apparatus-The-device-shown-is-a-full-body-magnetic.ppm} {Magnetic mocap suit}\relax }}{10}{figure.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Networks}{10}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Artificial Neural Network}{10}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \href  {https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/60d242974bcba9f8c670e03e_Group\%20806.jpg} {Neural Networks Architecture}\relax }}{11}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \href  {https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png} {Artificial Neuron}\relax }}{11}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \href  {https://datasciencepreparation.com/blog/articles/what-is-an-activation-function-what-are-commonly-used-activation-functions/} {Most common activation functions}\relax }}{12}{figure.caption.8}\protected@file@percent }
\citation{The importance of the loss function in option valuation}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \href  {https://www.guru99.com/images/1/030819_0937_BackPropaga1.png} {Backpropagation Algorithm}\relax }}{13}{figure.caption.9}\protected@file@percent }
\citation{Deep Learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Convolutional Neural Network}{14}{subsubsection.2.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \href  {https://www.researchgate.net/publication/336805909/figure/fig1/AS:817888827023360@1572011300751/Schematic-diagram-of-a-basic-convolutional-neural-network-CNN-architecture-26.ppm} {Basic Convolution Neural Network Architecture}\relax }}{15}{figure.caption.10}\protected@file@percent }
\citation{OpenPose}
\citation{HrNet}
\citation{AlphaPose}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Related Work}{16}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \href  {https://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.pdf} {2D Multi-Person high accuracy human Pose estimation}\relax }}{16}{figure.caption.11}\protected@file@percent }
\citation{Exploiting temporal information for 3D pose estimation}
\citation{3D Human Pose Estimation from Deep Multi-View 2D Pose}
\citation{3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information}
\citation{3D Human Pose Estimation = 2D Pose Estimation + Matching}
\citation{Human3.6M}
\citation{ANIMGAN}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \href  {https://arxiv.org/pdf/1612.06524.pdf} {2D and 3D human Pose difference in estimation}\relax }}{17}{figure.caption.12}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \href  {https://vision.imar.ro/human3.6m/pami-h36m.pdf} {Examples of human poses in the Human3.6M dataset}\relax }}{17}{figure.caption.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Gait mocap clip Generation through a GAN}{18}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Dataset}{18}{subsubsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Generative Adversarial Networks}{19}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \href  {https://bolster.ai/blog/content/images/2020/04/GAN-1.png} {A simple GAN architecture}\relax }}{19}{figure.caption.14}\protected@file@percent }
\citation{Human Action Generation with Generative Adversarial Networks}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Evaluation of the approach}{20}{subsubsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Requirements}{22}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Hardware}{22}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  MSI GL72M 7RDX performance stats\relax }}{22}{figure.caption.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Input requirements}{23}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Application's Workflow}{24}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces New input in the Application\relax }}{24}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Results Displayed in the Application\relax }}{24}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementation}{25}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Mocap Dataset Augmentation using pretrained models}{25}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Dataset Augmentation}{25}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Available Pretrained Models}{25}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Alpha-pose model}{25}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Orientation and location estimation}{25}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Animator Tools}{25}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Biovision Hierarchical (BVH)}{25}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Fast BVH Editing}{25}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}BVH Filtering}{25}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Windows Application}{25}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Application's UI}{25}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Tkinter library}{25}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Construction of the Application }{25}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{26}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Algorithm Accuracy}{26}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Comparing Result}{26}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{27}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Drawbacks of our Work}{27}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Future Approaches}{27}{subsection.6.2}\protected@file@percent }
\bibcite{Efficient Content-Based Retrieval of Motion Capture Data}{1}
\bibcite{Exploiting temporal information for 3D pose estimation}{2}
\bibcite{3D Human Pose Estimation from Deep Multi-View 2D Pose}{3}
\bibcite{3D Human Pose Estimation Using Convolutional Neural Networks with 2D Pose Information}{4}
\bibcite{Review on Motion Capture Technology}{5}
\bibcite{Optical Motion Capture: Theory and Implementation}{6}
\bibcite{Kalman Filtering for Sensor Fusionin a Human Tracking System}{7}
\bibcite{MOTION CAPTURE TO BUILD A FOUNDATION FOR A COMPUTER-CONTROLLED INSTRUMENT BY STUDY OF CLASSICAL GUITAR PERFORMANCE}{8}
\bibcite{Deep Learning}{9}
\bibcite{ANIMGAN: A SPATIOTEMPORALLY-CONDITIONED GENERATIVE ADVERSARIAL NETWORK FOR CHARACTER ANIMATION}{10}
\bibcite{3D Human Pose Estimation = 2D Pose Estimation + Matching}{11}
\bibcite{The importance of the loss function in option valuation}{12}
\bibcite{Human3.6M}{13}
\bibcite{OpenPose}{14}
\bibcite{HrNet}{15}
\bibcite{AlphaPose}{16}
\bibcite{ANIMGAN}{17}
\bibcite{Human Action Generation with Generative Adversarial Networks}{18}
\newlabel{EndOfText}{{6.2}{29}{Future Approaches}{subsection.6.2}{}}
\newlabel{endOfDoc}{{6.2}{29}{Future Approaches}{subsection.6.2}{}}
\newlabel{LastPage}{{}{29}{}{page.29}{}}
\xdef\lastpage@lastpage{29}
\xdef\lastpage@lastpageHy{29}
\gdef \@abspage@last{33}
